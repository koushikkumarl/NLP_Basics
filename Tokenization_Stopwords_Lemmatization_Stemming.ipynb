{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> <i>Paragraph we are using throughout this code: <br></i> </strong>\n",
    "I am honored to be with you today at your commencement from one of the finest universities in the world. I never graduated from college. Truth be told, this is the closest I’ve ever gotten to a college graduation. Today I want to tell you three stories from my life. That’s it. No big deal. Just three stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "#nltk.download() # Download all required packages for text processing \n",
    "\n",
    "paragraph = \"\"\"I am honored to be with you today at your commencement from one of the finest universities in the world. I never graduated from college. Truth be told, this is the closest I’ve ever gotten to a college graduation. \n",
    "Today I want to tell you three stories from my life. That’s it. No big deal. Just three stories.\"\"\"\n",
    "\n",
    "#If you are wondering why three quotes - Python's triple quotes helps by allowing strings to span multiple lines, \n",
    "#that includes new line characters, tab spaces or any special characters. \n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am honored to be with you today at your commencement from one of the finest universities in the world.',\n",
       " 'I never graduated from college.',\n",
       " 'Truth be told, this is the closest I’ve ever gotten to a college graduation.',\n",
       " 'Today I want to tell you three stories from my life.',\n",
       " 'That’s it.',\n",
       " 'No big deal.',\n",
       " 'Just three stories.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences #Paragraph tokenized as sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "#nltk.download() # Download all required packages for text processing \n",
    "\n",
    "paragraph = \"\"\"I am honored to be with you today at your commencement from one of the finest universities in the world. I never graduated from college. Truth be told, this is the closest I’ve ever gotten to a college graduation. \n",
    "Today I want to tell you three stories from my life. That’s it. No big deal. Just three stories.\"\"\"\n",
    "\n",
    "words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'honored', 'to', 'be', 'with', 'you', 'today', 'at', 'your', 'commencement', 'from', 'one', 'of', 'the', 'finest', 'universities', 'in', 'the', 'world', '.', 'I', 'never', 'graduated', 'from', 'college', '.', 'Truth', 'be', 'told', ',', 'this', 'is', 'the', 'closest', 'I', '’', 've', 'ever', 'gotten', 'to', 'a', 'college', 'graduation', '.', 'Today', 'I', 'want', 'to', 'tell', 'you', 'three', 'stories', 'from', 'my', 'life', '.', 'That', '’', 's', 'it', '.', 'No', 'big', 'deal', '.', 'Just', 'three', 'stories', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation for Regular Expressions, Stop words and Stemming / Lemmatization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I honor today commenc one finest univers world', 'I never graduat colleg', 'truth told closest I ever gotten colleg graduat', 'today I want tell three stori life', 'that', 'No big deal', 'just three stori']\n"
     ]
    }
   ],
   "source": [
    "## Stemming\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "\n",
    "paragraph = \"\"\"I am honored to be with you today at your commencement from one of the finest universities in the world. \n",
    "I never graduated from college. Truth be told, this is the closest I’ve ever gotten to a college graduation. \n",
    "Today I want to tell you three stories from my life. That’s it. No big deal. Just three stories.\"\"\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stemming\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)   \n",
    "    \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I honored today commencement one finest university world', 'I never graduated college', 'Truth told closest I ever gotten college graduation', 'Today I want tell three story life', 'That', 'No big deal', 'Just three story']\n"
     ]
    }
   ],
   "source": [
    "## Lemmatization\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "paragraph = \"\"\"I am honored to be with you today at your commencement from one of the finest universities in the world. \n",
    "I never graduated from college. Truth be told, this is the closest I’ve ever gotten to a college graduation. \n",
    "Today I want to tell you three stories from my life. That’s it. No big deal. Just three stories.\"\"\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Stemming\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)   \n",
    "    \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KerasEnv",
   "language": "python",
   "name": "kerasenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
